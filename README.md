# itdays-2019

Apache Spark is the de facto framework choice for big data processing, but learning it can be a little intimidating, due to its complexity.

The main role of Spark SQL is to reduce this complexity and to allow you to run queries on big data with a minimum learning effort. All you need to know is to write SQL queries !

In this workshop you will have a short introduction to Apache Spark, its architecture, data structures and after that we will focus on Spark SQL : create tables, investigate table schema,  write and run SQL queries, investigate query plans.

Workshop tools : Apache Spark, Python 3.7, Jupyter Notebook

How to install :
- clone the repo : https://github.com/tlapusan/itdays-2019.git (recommended to use an virtual environment)
- install all dependencies : pip install -r requirements.txt 
- start jupyter, from project root directory run './venv/bin/jupyter notebook'
- the notebook can be found at 'notebooks/bigdata/notebooks/Spark-SQL.ipynb'


