{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why do we need to analyze the data ?</b>\n",
    "- to understand our client's behaviour \n",
    "- to make business decisions based on data\n",
    "- to validate our business decisions\n",
    "\n",
    "<b>What tools are you using to analyze the data ?</b>\n",
    "- excel, sql, programming languages (java, python, etc), big data tools \n",
    "\n",
    "<b>What does Big Data mean for you ?</b>\n",
    "- Big Data can means a large volume of data, which cannot be stored and processed efficiently by traditional data management tools.\n",
    "\n",
    "<b>Why SQL on Big Data ? </b> \n",
    "- SQL is one of the most common skill. Almost any developer knows how to write a simple SQL query.\n",
    "- If a big data framework would support SQL, all of a sudden, everyone could do analysis on big data ! <br>\n",
    "\n",
    "There are many SQL tools for big data. Amazon Athena, redshift from Amazon, BigQuery from google. Hive, Spark SQL as open source tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/tlapusan/itdays-2019/blob/master/bigdata/resources/images/spark_logo.png?raw=true' />\n",
    "\n",
    "Most of the time we are used to code/work on a single machine (laptop). But there are moments in our developers' life  when a single machine is not powerful enough, especially when we are dealing with processing of a large volume of data. <br>\n",
    "One idea would be to use a cluster of machines and use all their resources (CPU, RAM, HDD). If we are talking about a cluster of machines to comunicate between each other, that means that we need networking, multitreading, etc skills...scary.\n",
    "An ideal scenario would be to have a framework to handle this hard work and to give us the impression that we are still working on a single machine. This is what Apache Spark does !\n",
    "\n",
    "\n",
    "Apache Spark is a distributed computing engine. It is able to process a large volume data, for tasks like batch or streaming processing, SQL, machine learning, graph processing. wow !\n",
    "\n",
    "How can we deploy/use it ? <br>\n",
    "\n",
    "Even if Spark looks like a very big framework, we can install it easily on our laptops and just start coding.  \n",
    "The best part...the code that we write on our laptops can be deployed and run on a clusters with hundreds of servers, whitout any changes ;) <br>\n",
    "Supported programming languages : Java, Scala, Python, R.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Spark\n",
    "We can install Spark for Python (pyspark) using pip package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init SparkSession\n",
    "SparkSession is the entry point for each Spark application. <br>\n",
    "When we instantiate a SparkSession, we create a driver process from where we can execute user-defined code on our big datasets.\n",
    "\n",
    "<img src='https://github.com/tlapusan/itdays-2019/blob/master/bigdata/resources/images/spark_application_architecture.png?raw=true'/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "            master(\"local[4]\").\\\n",
    "            appName(\"Spark-SQL\").\\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.123:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark-SQL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x119ff4690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data\n",
    "Spark can read/write data from a variaty of data formats, like csv, json, parquet, jdbc\n",
    "\n",
    "Dataset description : The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_bank = spark.read.parquet(\"../resources/data/parquet/client_bank/\")\n",
    "client_campaign = spark.read.parquet(\"../resources/data/parquet/client_campaing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(client_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe\n",
    "\n",
    "Dataframe is an immutable distributed table-like collection of data. It has a schema which defines the column names and data types.\n",
    "\n",
    "<img src='https://github.com/tlapusan/itdays-2019/blob/master/bigdata/resources/images/dataframe_structure.png?raw=true' width='70%'/>\n",
    " \n",
    "\n",
    "TODO\n",
    "- the role of immutability for dataframe ?\n",
    "- penalty of using python in spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+-------+-------------------+-------+-------+----+----------+\n",
      "| id|age|        job|marital|          education|default|housing|loan|subscribed|\n",
      "+---+---+-----------+-------+-------------------+-------+-------+----+----------+\n",
      "|  0| 56|  housemaid|married|           basic.4y|     no|     no|  no|        no|\n",
      "|  1| 57|   services|married|        high.school|unknown|     no|  no|        no|\n",
      "|  2| 37|   services|married|        high.school|     no|    yes|  no|        no|\n",
      "|  3| 40|     admin.|married|           basic.6y|     no|     no|  no|        no|\n",
      "|  4| 56|   services|married|        high.school|     no|     no| yes|        no|\n",
      "|  5| 45|   services|married|           basic.9y|unknown|     no|  no|        no|\n",
      "|  6| 59|     admin.|married|professional.course|     no|     no|  no|        no|\n",
      "|  7| 41|blue-collar|married|            unknown|unknown|     no|  no|        no|\n",
      "|  8| 24| technician| single|professional.course|     no|    yes|  no|        no|\n",
      "|  9| 25|   services| single|        high.school|     no|    yes|  no|        no|\n",
      "+---+---+-----------+-------+-------------------+-------+-------+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,IntegerType,true),StructField(age,IntegerType,true),StructField(job,StringType,true),StructField(marital,StringType,true),StructField(education,StringType,true),StructField(default,StringType,true),StructField(housing,StringType,true),StructField(loan,StringType,true),StructField(subscribed,StringType,true)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_bank.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- subscribed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data related with bank client information : <br>\n",
    "<b>id</b> - phone call id <br>\n",
    "<b>age</b> - client age <br>\n",
    "<b>job</b> - client job <br>\n",
    "<b>marital</b> - marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed) <br>\n",
    "<b>education</b> - client education 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown' <br>\n",
    "<b>default</b> - has credit in default? (categorical: 'no','yes','unknown'), default is failure to meet the legal obligations (or conditions) of a loan <br>\n",
    "<b>housing</b> -  has housing loan? (categorical: 'no','yes','unknown') <br>\n",
    "<b>loan</b> - has personal loan? (categorical: 'no','yes','unknown') <br>\n",
    "<b>subscribed</b> - if the client subscribed to the bank term deposit (categorical: 'no','yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+-----------+--------+\n",
      "| id|  contact|month|day_of_week|duration|\n",
      "+---+---------+-----+-----------+--------+\n",
      "|  0|telephone|  may|        mon|     261|\n",
      "|  1|telephone|  may|        mon|     149|\n",
      "|  2|telephone|  may|        mon|     226|\n",
      "|  3|telephone|  may|        mon|     151|\n",
      "|  4|telephone|  may|        mon|     307|\n",
      "|  5|telephone|  may|        mon|     198|\n",
      "|  6|telephone|  may|        mon|     139|\n",
      "|  7|telephone|  may|        mon|     217|\n",
      "|  8|telephone|  may|        mon|     380|\n",
      "|  9|telephone|  may|        mon|      50|\n",
      "+---+---------+-----+-----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data related with the phone call contact\n",
    "client_campaign.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_campaign.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>id</b> phone call id <br>\n",
    "<b>contact</b> - contact communication type (categorical: 'cellular','telephone') <br>\n",
    "<b>month</b> - contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec') <br>\n",
    "<b>day_of_week</b> - contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>\n",
    "<b>duration</b> - contact duration, in seconds (numeric). <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "Spark offers multiple ways to analyze data. The most commonly used are DataFrame API and Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column rename\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default_credit: string (nullable = true)\n",
      " |-- housing_loan: string (nullable = true)\n",
      " |-- personal_loan: string (nullable = true)\n",
      " |-- subscribed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank = client_bank.\\\n",
    "    withColumnRenamed(\"default\", \"default_credit\").\\\n",
    "    withColumnRenamed(\"housing\", \"housing_loan\").\\\n",
    "    withColumnRenamed(\"loan\", \"personal_loan\")\n",
    "\n",
    "client_bank.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise \n",
    "# rename column 'contact' into 'contact_type' for client_campaing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_campaign = client_campaign.withColumnRenamed(\"contact\", \"contact_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- contact_type: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_campaign.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select columns\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+--------+-------------------+--------------+------------+-------------+----------+\n",
      "| id|age|        job| marital|          education|default_credit|housing_loan|personal_loan|subscribed|\n",
      "+---+---+-----------+--------+-------------------+--------------+------------+-------------+----------+\n",
      "|  0| 56|  housemaid| married|           basic.4y|            no|          no|           no|        no|\n",
      "|  1| 57|   services| married|        high.school|       unknown|          no|           no|        no|\n",
      "|  2| 37|   services| married|        high.school|            no|         yes|           no|        no|\n",
      "|  3| 40|     admin.| married|           basic.6y|            no|          no|           no|        no|\n",
      "|  4| 56|   services| married|        high.school|            no|          no|          yes|        no|\n",
      "|  5| 45|   services| married|           basic.9y|       unknown|          no|           no|        no|\n",
      "|  6| 59|     admin.| married|professional.course|            no|          no|           no|        no|\n",
      "|  7| 41|blue-collar| married|            unknown|       unknown|          no|           no|        no|\n",
      "|  8| 24| technician|  single|professional.course|            no|         yes|           no|        no|\n",
      "|  9| 25|   services|  single|        high.school|            no|         yes|           no|        no|\n",
      "| 10| 41|blue-collar| married|            unknown|       unknown|          no|           no|        no|\n",
      "| 11| 25|   services|  single|        high.school|            no|         yes|           no|        no|\n",
      "| 12| 29|blue-collar|  single|        high.school|            no|          no|          yes|        no|\n",
      "| 13| 57|  housemaid|divorced|           basic.4y|            no|         yes|           no|        no|\n",
      "| 14| 35|blue-collar| married|           basic.6y|            no|         yes|           no|        no|\n",
      "| 15| 54|    retired| married|           basic.9y|       unknown|         yes|          yes|        no|\n",
      "| 16| 35|blue-collar| married|           basic.6y|            no|         yes|           no|        no|\n",
      "| 17| 46|blue-collar| married|           basic.6y|       unknown|         yes|          yes|        no|\n",
      "| 18| 50|blue-collar| married|           basic.9y|            no|         yes|          yes|        no|\n",
      "| 19| 39| management|  single|           basic.9y|       unknown|          no|           no|        no|\n",
      "+---+---+-----------+--------+-------------------+--------------+------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.\\\n",
    "    select(\"*\").\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------------------+----------+\n",
      "|age|        job|          education|subscribed|\n",
      "+---+-----------+-------------------+----------+\n",
      "| 56|  housemaid|           basic.4y|        no|\n",
      "| 57|   services|        high.school|        no|\n",
      "| 37|   services|        high.school|        no|\n",
      "| 40|     admin.|           basic.6y|        no|\n",
      "| 56|   services|        high.school|        no|\n",
      "| 45|   services|           basic.9y|        no|\n",
      "| 59|     admin.|professional.course|        no|\n",
      "| 41|blue-collar|            unknown|        no|\n",
      "| 24| technician|professional.course|        no|\n",
      "| 25|   services|        high.school|        no|\n",
      "| 41|blue-collar|            unknown|        no|\n",
      "| 25|   services|        high.school|        no|\n",
      "| 29|blue-collar|        high.school|        no|\n",
      "| 57|  housemaid|           basic.4y|        no|\n",
      "| 35|blue-collar|           basic.6y|        no|\n",
      "| 54|    retired|           basic.9y|        no|\n",
      "| 35|blue-collar|           basic.6y|        no|\n",
      "| 46|blue-collar|           basic.6y|        no|\n",
      "| 50|blue-collar|           basic.9y|        no|\n",
      "| 39| management|           basic.9y|        no|\n",
      "+---+-----------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.\\\n",
    "    select([\"age\", \"job\", \"education\", \"subscribed\"]).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+-------+-------------------+--------------+------------+-------------+----------+\n",
      "|   id|age|        job|marital|          education|default_credit|housing_loan|personal_loan|subscribed|\n",
      "+-----+---+-----------+-------+-------------------+--------------+------------+-------------+----------+\n",
      "|  568| 23|     admin.| single|  university.degree|            no|          no|           no|        no|\n",
      "| 2640| 23|blue-collar|married|professional.course|            no|         yes|           no|        no|\n",
      "| 2928| 23|   services| single|        high.school|            no|          no|           no|        no|\n",
      "| 2930| 23|   services| single|        high.school|            no|          no|           no|        no|\n",
      "| 3855| 23|   services|married|           basic.9y|            no|          no|           no|        no|\n",
      "| 5249| 23|   services|married|           basic.9y|            no|         yes|           no|        no|\n",
      "| 5910| 23|blue-collar|married|           basic.9y|            no|          no|           no|        no|\n",
      "| 7533| 23|   services| single|        high.school|            no|          no|           no|        no|\n",
      "| 7682| 23|     admin.|married|        high.school|            no|         yes|           no|        no|\n",
      "| 8557| 23|blue-collar| single|           basic.9y|            no|         yes|           no|        no|\n",
      "| 8649| 23|blue-collar|married|           basic.9y|            no|         yes|           no|        no|\n",
      "| 8679| 23|   services| single|        high.school|            no|          no|           no|        no|\n",
      "| 8763| 23|   services| single|professional.course|       unknown|          no|           no|        no|\n",
      "| 8808| 23|   services| single|professional.course|       unknown|          no|           no|        no|\n",
      "| 9062| 23|blue-collar|married|        high.school|            no|          no|          yes|        no|\n",
      "|10885| 23|blue-collar|married|professional.course|       unknown|          no|           no|        no|\n",
      "|11060| 23|blue-collar| single|           basic.9y|            no|         yes|           no|        no|\n",
      "|11231| 23|   services| single|        high.school|            no|          no|           no|        no|\n",
      "|11252| 23| technician| single|professional.course|            no|     unknown|      unknown|        no|\n",
      "|11275| 23|blue-collar| single|           basic.9y|            no|          no|           no|        no|\n",
      "+-----+---+-----------+-------+-------------------+--------------+------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.\\\n",
    "    where(F.col(\"age\") == 23).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+-------+-------------------+--------------+------------+-------------+----------+\n",
      "|   id|age|        job|marital|          education|default_credit|housing_loan|personal_loan|subscribed|\n",
      "+-----+---+-----------+-------+-------------------+--------------+------------+-------------+----------+\n",
      "|13807| 23|   services| single|        high.school|            no|          no|           no|       yes|\n",
      "|14220| 23|     admin.|married|           basic.9y|            no|         yes|           no|       yes|\n",
      "|14815| 23|blue-collar| single|        high.school|            no|          no|           no|       yes|\n",
      "|14821| 23|blue-collar| single|        high.school|            no|         yes|           no|       yes|\n",
      "|15126| 23| management| single|  university.degree|            no|          no|           no|       yes|\n",
      "|15187| 23| management| single|  university.degree|            no|          no|           no|       yes|\n",
      "|16621| 23|     admin.| single|professional.course|            no|         yes|           no|       yes|\n",
      "|18041| 23|   services|married|        high.school|            no|          no|           no|       yes|\n",
      "|27738| 23|     admin.| single|  university.degree|            no|     unknown|      unknown|       yes|\n",
      "|30017| 23|     admin.| single|  university.degree|            no|          no|           no|       yes|\n",
      "|30019| 23|     admin.| single|  university.degree|            no|         yes|           no|       yes|\n",
      "|30020| 23|     admin.| single|  university.degree|            no|         yes|           no|       yes|\n",
      "|30023| 23|     admin.| single|  university.degree|            no|          no|           no|       yes|\n",
      "|30105| 23| management| single|  university.degree|            no|         yes|           no|       yes|\n",
      "|32639| 23|blue-collar| single|           basic.9y|            no|         yes|           no|       yes|\n",
      "|32830| 23|blue-collar| single|           basic.9y|            no|         yes|           no|       yes|\n",
      "|34596| 23| technician| single|        high.school|            no|          no|          yes|       yes|\n",
      "|35049| 23|blue-collar| single|           basic.4y|            no|         yes|           no|       yes|\n",
      "|36423| 23|    student| single|        high.school|            no|          no|           no|       yes|\n",
      "|36548| 23|    student| single|        high.school|            no|          no|           no|       yes|\n",
      "+-----+---+-----------+-------+-------------------+--------------+------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.\\\n",
    "    where((F.col(\"age\") == 23) & (F.col(\"subscribed\") == \"yes\")).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|subscribed|count|\n",
      "+----------+-----+\n",
      "|        no|36548|\n",
      "|       yes| 4640|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how many phone calls were successful/unsuccessful ?\n",
    "client_bank.\\\n",
    "    groupBy(F.col(\"subscribed\")).\\\n",
    "    count().\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------+--------+\n",
      "|subscribed|call_count|min_age|max_age|mean_age|\n",
      "+----------+----------+-------+-------+--------+\n",
      "|        no|     36548|     17|     95|   39.91|\n",
      "|       yes|      4640|     17|     98|   40.91|\n",
      "+----------+----------+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.\\\n",
    "    groupBy(F.col(\"subscribed\")).\\\n",
    "    agg(\n",
    "        F.count(\"*\").alias(\"call_count\"),\n",
    "        F.min(\"age\").alias(\"min_age\"), \n",
    "        F.max(\"age\").alias(\"max_age\"),\n",
    "        F.round(F.mean(\"age\"),2).alias(\"mean_age\")\n",
    "    ).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order by\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+-----------+--------------+------------+-------------+----------+\n",
      "|   id|age|    job|marital|  education|default_credit|housing_loan|personal_loan|subscribed|\n",
      "+-----+---+-------+-------+-----------+--------------+------------+-------------+----------+\n",
      "|37558| 17|student| single|   basic.9y|            no|         yes|           no|        no|\n",
      "|37140| 17|student| single|    unknown|            no|         yes|           no|        no|\n",
      "|37579| 17|student| single|   basic.9y|            no|     unknown|      unknown|       yes|\n",
      "|37539| 17|student| single|   basic.9y|            no|         yes|           no|        no|\n",
      "|38274| 17|student| single|    unknown|            no|          no|          yes|       yes|\n",
      "|37934| 18|student| single|    unknown|            no|     unknown|      unknown|        no|\n",
      "|37916| 18|student| single|    unknown|            no|          no|           no|       yes|\n",
      "|37955| 18|student| single|    unknown|            no|         yes|           no|       yes|\n",
      "|38009| 18|student| single|    unknown|            no|          no|           no|        no|\n",
      "|27792| 18|student| single|high.school|            no|         yes|          yes|        no|\n",
      "|30142| 18|student| single|   basic.4y|            no|          no|           no|        no|\n",
      "|37626| 18|student| single|   basic.6y|            no|         yes|           no|        no|\n",
      "|39575| 18|student| single|    unknown|            no|         yes|           no|       yes|\n",
      "|39576| 18|student| single|    unknown|            no|         yes|           no|       yes|\n",
      "|39593| 18|student| single|    unknown|            no|          no|           no|        no|\n",
      "|35871| 18|student| single|high.school|            no|          no|           no|       yes|\n",
      "|38597| 18|student| single|   basic.6y|            no|          no|          yes|       yes|\n",
      "|38832| 18|student| single|high.school|            no|          no|           no|       yes|\n",
      "|41084| 18|student| single|    unknown|            no|         yes|           no|        no|\n",
      "|37917| 18|student| single|    unknown|            no|         yes|           no|        no|\n",
      "+-----+---+-------+-------+-----------+--------------+------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.\\\n",
    "    orderBy(F.col(\"age\"), ascending=True).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- contact_type: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_campaign.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+-------+-------------------+--------------+------------+-------------+----------+------------+-----+-----------+--------+\n",
      "| id|age|        job|marital|          education|default_credit|housing_loan|personal_loan|subscribed|contact_type|month|day_of_week|duration|\n",
      "+---+---+-----------+-------+-------------------+--------------+------------+-------------+----------+------------+-----+-----------+--------+\n",
      "|  0| 56|  housemaid|married|           basic.4y|            no|          no|           no|        no|   telephone|  may|        mon|     261|\n",
      "|  1| 57|   services|married|        high.school|       unknown|          no|           no|        no|   telephone|  may|        mon|     149|\n",
      "|  2| 37|   services|married|        high.school|            no|         yes|           no|        no|   telephone|  may|        mon|     226|\n",
      "|  3| 40|     admin.|married|           basic.6y|            no|          no|           no|        no|   telephone|  may|        mon|     151|\n",
      "|  4| 56|   services|married|        high.school|            no|          no|          yes|        no|   telephone|  may|        mon|     307|\n",
      "|  5| 45|   services|married|           basic.9y|       unknown|          no|           no|        no|   telephone|  may|        mon|     198|\n",
      "|  6| 59|     admin.|married|professional.course|            no|          no|           no|        no|   telephone|  may|        mon|     139|\n",
      "|  7| 41|blue-collar|married|            unknown|       unknown|          no|           no|        no|   telephone|  may|        mon|     217|\n",
      "|  8| 24| technician| single|professional.course|            no|         yes|           no|        no|   telephone|  may|        mon|     380|\n",
      "|  9| 25|   services| single|        high.school|            no|         yes|           no|        no|   telephone|  may|        mon|      50|\n",
      "+---+---+-----------+-------+-------------------+--------------+------------+-------------+----------+------------+-----+-----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.\\\n",
    "    join(client_campaign, [\"id\"], how=\"inner\").\\\n",
    "    show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------+-----+\n",
      "|age|          education| contact|month|\n",
      "+---+-------------------+--------+-----+\n",
      "| 45|           basic.9y|cellular|  jul|\n",
      "| 29|        high.school|cellular|  jul|\n",
      "| 33|           basic.9y|cellular|  jul|\n",
      "| 41|  university.degree|cellular|  jul|\n",
      "| 30|        high.school|cellular|  jul|\n",
      "| 58|professional.course|cellular|  jul|\n",
      "| 31|           basic.9y|cellular|  jul|\n",
      "| 30|        high.school|cellular|  jul|\n",
      "| 40|           basic.6y|cellular|  jul|\n",
      "| 33|        high.school|cellular|  jul|\n",
      "| 38|           basic.9y|cellular|  jul|\n",
      "| 29|           basic.9y|cellular|  jul|\n",
      "| 48|           basic.4y|cellular|  jul|\n",
      "| 51|           basic.9y|cellular|  jul|\n",
      "| 51|           basic.9y|cellular|  jul|\n",
      "| 34|        high.school|cellular|  jul|\n",
      "| 46|           basic.9y|cellular|  jul|\n",
      "| 34|        high.school|cellular|  jul|\n",
      "| 55|        high.school|cellular|  jul|\n",
      "| 47|  university.degree|cellular|  jul|\n",
      "+---+-------------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_bank.\\\n",
    "    join(client_campaign, [\"id\"], how=\"inner\").\\\n",
    "    select([\"age\", \"education\", \"contact\", \"month\"]).\\\n",
    "    where(F.col(\"contact\") == \"cellular\").\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "To run any type of SQL query, we need first to create a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_bank.createOrReplaceTempView(\"client_bank\")\n",
    "client_campaign.createOrReplaceTempView(\"client_campaign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------+--------+-------------------+--------------+------------+-------------+----------+\n",
      "| id|age|        job| marital|          education|default_credit|housing_loan|personal_loan|subscribed|\n",
      "+---+---+-----------+--------+-------------------+--------------+------------+-------------+----------+\n",
      "|  0| 56|  housemaid| married|           basic.4y|            no|          no|           no|        no|\n",
      "|  1| 57|   services| married|        high.school|       unknown|          no|           no|        no|\n",
      "|  2| 37|   services| married|        high.school|            no|         yes|           no|        no|\n",
      "|  3| 40|     admin.| married|           basic.6y|            no|          no|           no|        no|\n",
      "|  4| 56|   services| married|        high.school|            no|          no|          yes|        no|\n",
      "|  5| 45|   services| married|           basic.9y|       unknown|          no|           no|        no|\n",
      "|  6| 59|     admin.| married|professional.course|            no|          no|           no|        no|\n",
      "|  7| 41|blue-collar| married|            unknown|       unknown|          no|           no|        no|\n",
      "|  8| 24| technician|  single|professional.course|            no|         yes|           no|        no|\n",
      "|  9| 25|   services|  single|        high.school|            no|         yes|           no|        no|\n",
      "| 10| 41|blue-collar| married|            unknown|       unknown|          no|           no|        no|\n",
      "| 11| 25|   services|  single|        high.school|            no|         yes|           no|        no|\n",
      "| 12| 29|blue-collar|  single|        high.school|            no|          no|          yes|        no|\n",
      "| 13| 57|  housemaid|divorced|           basic.4y|            no|         yes|           no|        no|\n",
      "| 14| 35|blue-collar| married|           basic.6y|            no|         yes|           no|        no|\n",
      "| 15| 54|    retired| married|           basic.9y|       unknown|         yes|          yes|        no|\n",
      "| 16| 35|blue-collar| married|           basic.6y|            no|         yes|           no|        no|\n",
      "| 17| 46|blue-collar| married|           basic.6y|       unknown|         yes|          yes|        no|\n",
      "| 18| 50|blue-collar| married|           basic.9y|            no|         yes|          yes|        no|\n",
      "| 19| 39| management|  single|           basic.9y|       unknown|          no|           no|        no|\n",
      "+---+---+-----------+--------+-------------------+--------------+------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM client_bank\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|        job|\n",
      "+---+-----------+\n",
      "| 56|  housemaid|\n",
      "| 57|   services|\n",
      "| 37|   services|\n",
      "| 40|     admin.|\n",
      "| 56|   services|\n",
      "| 45|   services|\n",
      "| 59|     admin.|\n",
      "| 41|blue-collar|\n",
      "| 24| technician|\n",
      "| 25|   services|\n",
      "| 41|blue-collar|\n",
      "| 25|   services|\n",
      "| 29|blue-collar|\n",
      "| 57|  housemaid|\n",
      "| 35|blue-collar|\n",
      "| 54|    retired|\n",
      "| 35|blue-collar|\n",
      "| 46|blue-collar|\n",
      "| 50|blue-collar|\n",
      "| 39| management|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT age, job FROM client_bank\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------+--------+-------------------+--------------+------------+-------------+----------+\n",
      "|  id|age|          job| marital|          education|default_credit|housing_loan|personal_loan|subscribed|\n",
      "+----+---+-------------+--------+-------------------+--------------+------------+-------------+----------+\n",
      "|  89| 34|       admin.| married|        high.school|            no|         yes|           no|        no|\n",
      "| 106| 34|    housemaid| married|           basic.6y|            no|         yes|           no|        no|\n",
      "| 138| 34|     services| married|        high.school|            no|         yes|           no|        no|\n",
      "| 189| 34|       admin.| married|  university.degree|            no|         yes|           no|        no|\n",
      "| 194| 34|  blue-collar|  single|           basic.9y|       unknown|         yes|           no|        no|\n",
      "| 242| 34|   management| married|  university.degree|            no|         yes|           no|        no|\n",
      "| 266| 34|self-employed|  single|  university.degree|            no|         yes|           no|        no|\n",
      "| 419| 34|   technician|  single|  university.degree|            no|         yes|           no|        no|\n",
      "| 490| 34|   technician| married|  university.degree|            no|         yes|           no|        no|\n",
      "| 548| 34|       admin.|  single|  university.degree|            no|         yes|          yes|        no|\n",
      "| 642| 34|  blue-collar| married|           basic.4y|            no|         yes|           no|        no|\n",
      "| 847| 34|       admin.|  single|        high.school|            no|         yes|           no|        no|\n",
      "| 862| 34|   technician| married|professional.course|       unknown|         yes|           no|        no|\n",
      "| 900| 34|   technician|divorced|professional.course|            no|         yes|           no|        no|\n",
      "| 939| 34|   technician| married|professional.course|       unknown|         yes|           no|        no|\n",
      "| 979| 34| entrepreneur| married|           basic.4y|            no|         yes|           no|        no|\n",
      "| 984| 34|  blue-collar| married|           basic.6y|            no|         yes|           no|        no|\n",
      "| 986| 34|  blue-collar| married|           basic.6y|            no|         yes|           no|        no|\n",
      "|1105| 34|   management| married|  university.degree|            no|         yes|           no|        no|\n",
      "|1107| 34|   management| married|  university.degree|            no|         yes|          yes|        no|\n",
      "+----+---+-------------+--------+-------------------+--------------+------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * \n",
    "            FROM client_bank\n",
    "            WHERE age == 34 AND housing_loan == 'yes'\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|          job|count|\n",
      "+-------------+-----+\n",
      "|   management|    1|\n",
      "|self-employed|    2|\n",
      "|      student|   54|\n",
      "|  blue-collar|   15|\n",
      "|       admin.|   12|\n",
      "|   technician|    2|\n",
      "|     services|   12|\n",
      "|    housemaid|    1|\n",
      "|   unemployed|    3|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT job, count(*) AS count\n",
    "            FROM client_bank\n",
    "            WHERE AGE == 21\n",
    "            GROUP BY job\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+--------+\n",
      "|          job|count|mean_age|\n",
      "+-------------+-----+--------+\n",
      "|   management| 2596|   42.31|\n",
      "|      retired| 1286|  59.926|\n",
      "|      unknown|  293|  45.375|\n",
      "|self-employed| 1272|  40.177|\n",
      "|      student|  600|  26.397|\n",
      "|  blue-collar| 8616|  39.582|\n",
      "| entrepreneur| 1332|  41.703|\n",
      "|       admin.| 9070|   38.22|\n",
      "|   technician| 6013|    38.6|\n",
      "|     services| 3646|   38.09|\n",
      "|    housemaid|  954|  44.705|\n",
      "|   unemployed|  870|  39.845|\n",
      "+-------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT job, count(*) AS count, round(mean(age), 3) AS mean_age\n",
    "            FROM client_bank\n",
    "            WHERE subscribed == 'no'\n",
    "            GROUP BY job\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+-------+-----------+--------------+------------+-------------+----------+\n",
      "|   id|age|    job|marital|  education|default_credit|housing_loan|personal_loan|subscribed|\n",
      "+-----+---+-------+-------+-----------+--------------+------------+-------------+----------+\n",
      "|37558| 17|student| single|   basic.9y|            no|         yes|           no|        no|\n",
      "|37140| 17|student| single|    unknown|            no|         yes|           no|        no|\n",
      "|37579| 17|student| single|   basic.9y|            no|     unknown|      unknown|       yes|\n",
      "|37539| 17|student| single|   basic.9y|            no|         yes|           no|        no|\n",
      "|38274| 17|student| single|    unknown|            no|          no|          yes|       yes|\n",
      "|37934| 18|student| single|    unknown|            no|     unknown|      unknown|        no|\n",
      "|37916| 18|student| single|    unknown|            no|          no|           no|       yes|\n",
      "|37955| 18|student| single|    unknown|            no|         yes|           no|       yes|\n",
      "|38009| 18|student| single|    unknown|            no|          no|           no|        no|\n",
      "|27792| 18|student| single|high.school|            no|         yes|          yes|        no|\n",
      "|30142| 18|student| single|   basic.4y|            no|          no|           no|        no|\n",
      "|37626| 18|student| single|   basic.6y|            no|         yes|           no|        no|\n",
      "|39575| 18|student| single|    unknown|            no|         yes|           no|       yes|\n",
      "|39576| 18|student| single|    unknown|            no|         yes|           no|       yes|\n",
      "|39593| 18|student| single|    unknown|            no|          no|           no|        no|\n",
      "|35871| 18|student| single|high.school|            no|          no|           no|       yes|\n",
      "|38597| 18|student| single|   basic.6y|            no|          no|          yes|       yes|\n",
      "|38832| 18|student| single|high.school|            no|          no|           no|       yes|\n",
      "|41084| 18|student| single|    unknown|            no|         yes|           no|        no|\n",
      "|37917| 18|student| single|    unknown|            no|         yes|           no|        no|\n",
      "+-----+---+-------+-------+-----------+--------------+------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * \n",
    "            FROM client_bank\n",
    "            ORDER BY age ASC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------------------+---------+\n",
      "| id| id|age|          education|  contact|\n",
      "+---+---+---+-------------------+---------+\n",
      "|  0|  0| 56|           basic.4y|telephone|\n",
      "|  1|  1| 57|        high.school|telephone|\n",
      "|  2|  2| 37|        high.school|telephone|\n",
      "|  3|  3| 40|           basic.6y|telephone|\n",
      "|  4|  4| 56|        high.school|telephone|\n",
      "|  5|  5| 45|           basic.9y|telephone|\n",
      "|  6|  6| 59|professional.course|telephone|\n",
      "|  7|  7| 41|            unknown|telephone|\n",
      "|  8|  8| 24|professional.course|telephone|\n",
      "|  9|  9| 25|        high.school|telephone|\n",
      "| 10| 10| 41|            unknown|telephone|\n",
      "| 11| 11| 25|        high.school|telephone|\n",
      "| 12| 12| 29|        high.school|telephone|\n",
      "| 13| 13| 57|           basic.4y|telephone|\n",
      "| 14| 14| 35|           basic.6y|telephone|\n",
      "| 15| 15| 54|           basic.9y|telephone|\n",
      "| 16| 16| 35|           basic.6y|telephone|\n",
      "| 17| 17| 46|           basic.6y|telephone|\n",
      "| 18| 18| 50|           basic.9y|telephone|\n",
      "| 19| 19| 39|           basic.9y|telephone|\n",
      "+---+---+---+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT cb.id, cc.id, cb.age, cb.education, cc.contact\n",
    "            FROM client_bank AS cb INNER JOIN client_campaign AS cc ON cb.id == cc.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subselect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+\n",
      "|age|          education|count|\n",
      "+---+-------------------+-----+\n",
      "| 98|           basic.4y|    2|\n",
      "| 95|           basic.6y|    1|\n",
      "| 94|           basic.9y|    1|\n",
      "| 92|            unknown|    4|\n",
      "| 91|  university.degree|    2|\n",
      "| 89|           basic.4y|    2|\n",
      "| 88|           basic.4y|   20|\n",
      "| 88|  university.degree|    1|\n",
      "| 88|        high.school|    1|\n",
      "| 87|           basic.4y|    1|\n",
      "| 86|            unknown|    2|\n",
      "| 86|           basic.4y|    4|\n",
      "| 86|           basic.9y|    1|\n",
      "| 86|professional.course|    1|\n",
      "| 85|           basic.4y|   13|\n",
      "| 85|professional.course|    2|\n",
      "| 84|        high.school|    1|\n",
      "| 84|           basic.4y|    4|\n",
      "| 84|            unknown|    1|\n",
      "| 84|           basic.9y|    1|\n",
      "+---+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT age, education, count(*) as count\n",
    "            FROM (\n",
    "                SELECT cb.id, cc.id, cb.age, cb.education, cc.contact\n",
    "                FROM client_bank AS cb INNER JOIN client_campaign AS cc ON cb.id == cc.id\n",
    "                )\n",
    "            GROUP BY age, education\n",
    "            ORDER BY age DESC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine SQL with Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+\n",
      "|age|     job|  education|\n",
      "+---+--------+-----------+\n",
      "| 37|services|high.school|\n",
      "| 25|services|high.school|\n",
      "| 25|services|high.school|\n",
      "| 34|services|high.school|\n",
      "| 45|services|high.school|\n",
      "| 33|services|high.school|\n",
      "| 43|services|high.school|\n",
      "| 35|services|high.school|\n",
      "| 41|services|high.school|\n",
      "| 34|services|high.school|\n",
      "| 39|services|high.school|\n",
      "| 39|services|high.school|\n",
      "| 38|services|high.school|\n",
      "| 36|services|high.school|\n",
      "| 43|services|high.school|\n",
      "| 44|services|high.school|\n",
      "| 28|services|high.school|\n",
      "| 33|services|high.school|\n",
      "| 40|services|high.school|\n",
      "| 33|services|high.school|\n",
      "+---+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT age, job, education\n",
    "            FROM client_bank\n",
    "            WHERE job == 'services'\"\"\").\\\n",
    "    where(F.col(\"age\").between(20,50)).\\\n",
    "    where(F.col(\"education\") == \"high.school\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL clients\n",
    "https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html\n",
    "\n",
    "- SparkSession\n",
    "- JDBC/ODBC ([BI tools](https://docs.databricks.com/bi/index.html))\n",
    "- command-line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Spark can run SQL code ?\n",
    "An SQL query declares our intentions, but it does not tell the exact logic flow to run. Spark needs to convert the SQL in a query plan, which is a set of steps of executions. <br>\n",
    "This process was not invented by Spark, it happens in all SQL servers.\n",
    "\n",
    "<img src='https://github.com/tlapusan/itdays-2019/blob/master/bigdata/resources/images/sql_catalyst.png?raw=true'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Parsed logical</b> plan checks code syntax. <br>\n",
    "<b>Analyzed logical plan</b> checks for tables, columns validity. <br>\n",
    "<b>Optimized logical plan</b> tries to apply optimisations on the logical plan, like pushing down predicates or column selections. <br>\n",
    "<b>Physical plan</b> specifies exactly how the plan will be executed on the cluster.\n",
    "\n",
    "<img src='https://github.com/tlapusan/itdays-2019/blob/master/bigdata/resources/images/query_plan_states.png?raw=true'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['id, 'age, 'education]\n",
      "+- 'UnresolvedRelation `client_bank`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, age: int, education: string\n",
      "Project [id#0, age#1, education#4]\n",
      "+- SubqueryAlias `client_bank`\n",
      "   +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing_loan#162, loan#7 AS personal_loan#172, subscribed#8]\n",
      "      +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing#6 AS housing_loan#162, loan#7, subscribed#8]\n",
      "         +- Project [id#0, age#1, job#2, marital#3, education#4, default#5 AS default_credit#152, housing#6, loan#7, subscribed#8]\n",
      "            +- Relation[id#0,age#1,job#2,marital#3,education#4,default#5,housing#6,loan#7,subscribed#8] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#0, age#1, education#4]\n",
      "+- Relation[id#0,age#1,job#2,marital#3,education#4,default#5,housing#6,loan#7,subscribed#8] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#0, age#1, education#4]\n",
      "+- *(1) FileScan parquet [id#0,age#1,education#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/tudorlapusan/Documents/workspaces/workshops/itdays-2019/bigdata/res..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,age:int,education:string>\n"
     ]
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.streaming.StreamingQuery.explain\n",
    "spark.sql(\"SELECT id, age, education FROM client_bank\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for parsed logical plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nextraneous input 'client_bank' expecting <EOF>(line 1, pos 32)\\n\\n== SQL ==\\nSELECT id, age, education FROM2 client_bank\\n--------------------------------^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nextraneous input 'client_bank' expecting <EOF>(line 1, pos 32)\n\n== SQL ==\nSELECT id, age, education FROM2 client_bank\n--------------------------------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-113f53d74ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT id, age, education FROM2 client_bank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nextraneous input 'client_bank' expecting <EOF>(line 1, pos 32)\\n\\n== SQL ==\\nSELECT id, age, education FROM2 client_bank\\n--------------------------------^^^\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, age, education FROM2 client_bank\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for analyzed logical plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`id2`' given input columns: [client_bank.personal_loan, client_bank.id, client_bank.default_credit, client_bank.education, client_bank.job, client_bank.age, client_bank.marital, client_bank.housing_loan, client_bank.subscribed]; line 1 pos 7;\\n'Project ['id2, age#1, education#4]\\n+- SubqueryAlias `client_bank`\\n   +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing_loan#162, loan#7 AS personal_loan#172, subscribed#8]\\n      +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing#6 AS housing_loan#162, loan#7, subscribed#8]\\n         +- Project [id#0, age#1, job#2, marital#3, education#4, default#5 AS default_credit#152, housing#6, loan#7, subscribed#8]\\n            +- Relation[id#0,age#1,job#2,marital#3,education#4,default#5,housing#6,loan#7,subscribed#8] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`id2`' given input columns: [client_bank.personal_loan, client_bank.id, client_bank.default_credit, client_bank.education, client_bank.job, client_bank.age, client_bank.marital, client_bank.housing_loan, client_bank.subscribed]; line 1 pos 7;\n'Project ['id2, age#1, education#4]\n+- SubqueryAlias `client_bank`\n   +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing_loan#162, loan#7 AS personal_loan#172, subscribed#8]\n      +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing#6 AS housing_loan#162, loan#7, subscribed#8]\n         +- Project [id#0, age#1, job#2, marital#3, education#4, default#5 AS default_credit#152, housing#6, loan#7, subscribed#8]\n            +- Relation[id#0,age#1,job#2,marital#3,education#4,default#5,housing#6,loan#7,subscribed#8] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-d6d222e1ab24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT id2, age, education FROM client_bank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/workshops/itdays-2019/venv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`id2`' given input columns: [client_bank.personal_loan, client_bank.id, client_bank.default_credit, client_bank.education, client_bank.job, client_bank.age, client_bank.marital, client_bank.housing_loan, client_bank.subscribed]; line 1 pos 7;\\n'Project ['id2, age#1, education#4]\\n+- SubqueryAlias `client_bank`\\n   +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing_loan#162, loan#7 AS personal_loan#172, subscribed#8]\\n      +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing#6 AS housing_loan#162, loan#7, subscribed#8]\\n         +- Project [id#0, age#1, job#2, marital#3, education#4, default#5 AS default_credit#152, housing#6, loan#7, subscribed#8]\\n            +- Relation[id#0,age#1,job#2,marital#3,education#4,default#5,housing#6,loan#7,subscribed#8] parquet\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id2, age, education FROM client_bank\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimized logical plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['id, 'age, 'education]\n",
      "+- 'Filter ('age = 3)\n",
      "   +- 'UnresolvedRelation `client_bank`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, age: int, education: string\n",
      "Project [id#0, age#1, education#4]\n",
      "+- Filter (age#1 = 3)\n",
      "   +- SubqueryAlias `client_bank`\n",
      "      +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing_loan#162, loan#7 AS personal_loan#172, subscribed#8]\n",
      "         +- Project [id#0, age#1, job#2, marital#3, education#4, default_credit#152, housing#6 AS housing_loan#162, loan#7, subscribed#8]\n",
      "            +- Project [id#0, age#1, job#2, marital#3, education#4, default#5 AS default_credit#152, housing#6, loan#7, subscribed#8]\n",
      "               +- Relation[id#0,age#1,job#2,marital#3,education#4,default#5,housing#6,loan#7,subscribed#8] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#0, age#1, education#4]\n",
      "+- Filter (isnotnull(age#1) && (age#1 = 3))\n",
      "   +- Relation[id#0,age#1,job#2,marital#3,education#4,default#5,housing#6,loan#7,subscribed#8] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [id#0, age#1, education#4]\n",
      "+- *(1) Filter (isnotnull(age#1) && (age#1 = 3))\n",
      "   +- *(1) FileScan parquet [id#0,age#1,education#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/tudorlapusan/Documents/workspaces/workshops/itdays-2019/bigdata/res..., PartitionFilters: [], PushedFilters: [IsNotNull(age), EqualTo(age,3)], ReadSchema: struct<id:int,age:int,education:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT id, age, education \n",
    "            FROM client_bank\n",
    "            WHERE age == 3\"\"\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API vs SQL query plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [age#1, job#2, marital#3, education#4]\n",
      "+- *(1) Filter (((isnotnull(age#1) && isnotnull(job#2)) && (age#1 > 30)) && (job#2 = management))\n",
      "   +- *(1) FileScan parquet [age#1,job#2,marital#3,education#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/tudorlapusan/Documents/workspaces/workshops/itdays-2019/bigdata/res..., PartitionFilters: [], PushedFilters: [IsNotNull(age), IsNotNull(job), GreaterThan(age,30), EqualTo(job,management)], ReadSchema: struct<age:int,job:string,marital:string,education:string>\n"
     ]
    }
   ],
   "source": [
    "client_bank.select([\"age\",\"job\",\"marital\",\"education\"]).\\\n",
    "    where(F.col(\"age\") > 30).\\\n",
    "    where(F.col(\"job\") == \"management\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [age#1, job#2, marital#3, education#4]\n",
      "+- *(1) Filter (((isnotnull(age#1) && isnotnull(job#2)) && (age#1 > 30)) && (job#2 = management))\n",
      "   +- *(1) FileScan parquet [age#1,job#2,marital#3,education#4] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/tudorlapusan/Documents/workspaces/workshops/itdays-2019/bigdata/res..., PartitionFilters: [], PushedFilters: [IsNotNull(age), IsNotNull(job), GreaterThan(age,30), EqualTo(job,management)], ReadSchema: struct<age:int,job:string,marital:string,education:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT age, job, marital, education\n",
    "        FROM client_bank\n",
    "        WHERE age > 30 AND job =='management'\"\"\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL vs RDBMS\n",
    "Why can't we use databases with lots of disks and CPUs to do large scale analytics ? <br>\n",
    "The answer comes from another trend in disk drives : seek time is improving more slowly than transfer rate.\n",
    "\n",
    "If the data access pattern is dominated by seeks, it will take longer to write/read the data then streaming through it, and vice versa.\n",
    "\n",
    "In many ways, Spark SQL and RDBMS complement each other. Spark SQL is very good for analysing the whole dataset (ad hoc queries) and RDBMS is good for point queries or updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark operations\n",
    "In Spark we have two types of operations : transformations and actions. <br>\n",
    "Transformations are those operations used to express the business logic of a spark application. <br>\n",
    "Actions are those operations used to trigger a pipeline of transformations.\n",
    "\n",
    "Lazy evaluation : spark will compute all the transformations, only in the last minute, when you actually call an action on them. In this way, Spark can look at the whole set of transformation and will try to apply optimisations on it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: int, age: int, job: string, marital: string, education: string, default: string, housing: string, loan: string, subscribed: string, subscribed_int: int]\n",
      "52.8 ms  0 ns per loop (mean  std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# tranformation\n",
    "%timeit -r1 -n1 print(client_bank.\\\n",
    "                            where(F.col(\"age\")==24)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n",
      "142 ms  0 ns per loop (mean  std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# action\n",
    "%timeit -r1 -n1 print(client_bank.\\\n",
    "                            where(F.col(\"age\")==24).\\\n",
    "                            count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL functions\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL resources\n",
    "Book - https://www.amazon.com/Spark-Definitive-Guide-Processing-Simple/dp/1491912219 <br>\n",
    "Videos :\n",
    "- https://databricks.com/sparkaisummit/north-america/sessions\n",
    "- https://databricks.com/session/from-basic-to-advanced-aggregate-operators-in-apache-spark-sql-2-2-by-examples-and-their-catalyst-optimizations-continues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project push down, filter push down, and partition pruning : https://drill.apache.org/docs/parquet-filter-pushdown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272.76px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
